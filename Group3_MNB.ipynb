{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('combined_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_stop_words = ['ہے', 'اور', 'کو', 'میں', 'یہ', 'سے', 'کہ', 'کا', 'کی', 'ایک']\n",
    "\n",
    "def clean_urdu_data(sentence):\n",
    "    urls = r'http\\S+|www\\S+|https\\S+' \n",
    "    non_urdu = r'[^\\u0600-\\u06FF\\s]' \n",
    "    whitespaces = r'\\s+'           \n",
    "    sentence = re.sub(urls, '', sentence)\n",
    "    sentence = re.sub(non_urdu, '', sentence)\n",
    "    sentence = re.sub(whitespaces, ' ', sentence).strip()\n",
    "    filtered_words = [word for word in sentence.split() if word not in urdu_stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "news_data['cleaned_content'] = news_data['content'].apply(lambda x: clean_urdu_data(x) if pd.notnull(x) else x)\n",
    "news_data = news_data[news_data['cleaned_content'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting X and Y and splitting test and train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data.dropna(subset=['cleaned_content'])\n",
    "X = news_data['cleaned_content']\n",
    "y = news_data['gold_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words class to vectorize the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = [] \n",
    "        self.word_dic = {}   \n",
    "\n",
    "    def fit(self, sentences):\n",
    "        word_set = set()\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            word_set.update(words)\n",
    "        self.vocabulary = sorted(word_set)\n",
    "        self.word_dic = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "\n",
    "    def vectorize(self, sentence):\n",
    "        vector = [0] * len(self.vocabulary) \n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            if word in self.word_dic: \n",
    "                index = self.word_dic[word]\n",
    "                vector[index] += 1 \n",
    "        return vector\n",
    "\n",
    "    def transform(self, sentences):\n",
    "        return [self.vectorize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "    def __init__(self):\n",
    "        self.priors = {}\n",
    "        self.conditional_probabilities = {}\n",
    "        self.classes = []\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def calculate_priors(self, y_train):\n",
    "        total = len(y_train)\n",
    "        self.classes, counts = np.unique(y_train, return_counts=True)\n",
    "        self.priors = {c: count / total for c, count in zip(self.classes, counts)}\n",
    "\n",
    "    def calculate_word_counts_per_class(self, X_train, y_train):\n",
    "        y_train = np.array(y_train)\n",
    "        word_counts = {c: np.zeros(self.vocab_size) for c in self.classes}\n",
    "        class_totals = {c: 0 for c in self.classes}\n",
    "        for i, vector in enumerate(X_train):\n",
    "            label = y_train[i]\n",
    "            word_counts[label] += vector\n",
    "            class_totals[label] += sum(vector)\n",
    "        return word_counts, class_totals\n",
    "\n",
    "    def calculate_conditional_probabilities(self, word_counts, total_word_counts):\n",
    "        for c in self.classes:\n",
    "            counts = word_counts[c]\n",
    "            total = total_word_counts[c]\n",
    "            self.conditional_probabilities[c] = (counts + 1) / (total + self.vocab_size)\n",
    "\n",
    "    def fit(self, X_train, y_train, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.calculate_priors(y_train)\n",
    "        word_counts, total_counts = self.calculate_word_counts_per_class(X_train, y_train)\n",
    "        self.calculate_conditional_probabilities(word_counts, total_counts)\n",
    "\n",
    "    def predict(self, vector):\n",
    "        log_probs = {}\n",
    "        for c in self.classes:\n",
    "            log_prob = np.log(self.priors[c])\n",
    "            for i in range(self.vocab_size):\n",
    "                if vector[i] != 0:\n",
    "                    log_prob += vector[i] * np.log(self.conditional_probabilities[c][i])\n",
    "            log_probs[c] = log_prob\n",
    "        return max(log_probs, key=log_probs.get)\n",
    "\n",
    "    def predict_batch(self, X_data):\n",
    "        return [self.predict(vector) for vector in X_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Bag-of-Words and fitting Multinomial model to train data and then predicting the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['science-technology', 'business', 'science-technology', 'world', 'science-technology', 'science-technology', 'entertainment', 'world', 'business', 'world', 'business', 'business', 'entertainment', 'sports', 'world', 'world', 'business', 'entertainment', 'entertainment', 'business', 'business', 'business', 'entertainment', 'world', 'world', 'business', 'science-technology', 'entertainment', 'business', 'sports', 'science-technology', 'entertainment', 'business', 'world', 'entertainment', 'business', 'world', 'sports', 'science-technology', 'business', 'sports', 'business', 'world', 'sports', 'entertainment', 'business', 'science-technology', 'business', 'business', 'entertainment', 'business', 'science-technology', 'world', 'world', 'entertainment', 'world', 'science-technology', 'business', 'sports', 'world', 'science-technology', 'sports', 'world', 'sports', 'science-technology', 'world', 'entertainment', 'sports', 'entertainment', 'sports', 'sports', 'entertainment', 'entertainment', 'science-technology', 'entertainment', 'world', 'science-technology', 'business', 'world', 'entertainment', 'sports', 'world', 'sports', 'sports', 'entertainment', 'science-technology', 'business', 'sports', 'entertainment', 'entertainment', 'science-technology', 'entertainment', 'business', 'entertainment', 'entertainment', 'world', 'science-technology', 'sports', 'business', 'sports', 'business', 'science-technology', 'business', 'entertainment', 'entertainment', 'world', 'sports', 'world', 'entertainment', 'entertainment', 'business', 'entertainment', 'world', 'world', 'business', 'business', 'business', 'business', 'sports', 'entertainment', 'science-technology', 'science-technology', 'science-technology', 'entertainment', 'world', 'sports', 'world', 'science-technology', 'business', 'science-technology', 'science-technology', 'world', 'world', 'business', 'business', 'science-technology', 'business', 'science-technology', 'sports', 'science-technology', 'world', 'world', 'world', 'sports', 'entertainment', 'world', 'entertainment', 'sports', 'world', 'sports', 'business', 'sports', 'world', 'sports', 'business', 'world', 'world', 'business', 'entertainment', 'science-technology', 'sports', 'science-technology', 'entertainment', 'business', 'business', 'world', 'world', 'sports', 'sports', 'science-technology', 'entertainment', 'entertainment', 'entertainment', 'science-technology', 'sports', 'entertainment', 'sports', 'world', 'science-technology', 'science-technology', 'entertainment', 'world', 'entertainment', 'business', 'science-technology', 'world', 'sports', 'business', 'business', 'sports', 'world', 'entertainment', 'world', 'science-technology', 'entertainment', 'science-technology', 'entertainment', 'entertainment', 'business', 'sports', 'science-technology', 'entertainment', 'world', 'science-technology', 'sports', 'sports', 'world', 'science-technology', 'sports', 'business', 'entertainment', 'entertainment', 'sports', 'entertainment', 'science-technology', 'world', 'entertainment', 'sports', 'science-technology', 'sports', 'sports', 'entertainment', 'sports', 'business', 'sports', 'entertainment', 'sports', 'entertainment', 'business', 'science-technology', 'entertainment', 'science-technology', 'science-technology', 'sports', 'business', 'business', 'entertainment', 'science-technology', 'world', 'sports', 'entertainment', 'entertainment', 'world', 'entertainment', 'sports', 'science-technology', 'entertainment', 'science-technology', 'sports', 'science-technology', 'science-technology', 'science-technology', 'business', 'world', 'science-technology', 'world', 'business', 'sports', 'business', 'entertainment', 'business', 'sports', 'world', 'science-technology', 'entertainment', 'world', 'entertainment', 'business', 'entertainment', 'science-technology', 'science-technology', 'business', 'business', 'science-technology', 'business', 'entertainment', 'business', 'business', 'world', 'entertainment', 'sports', 'science-technology', 'entertainment', 'world', 'science-technology', 'entertainment', 'entertainment', 'world', 'world', 'business', 'entertainment', 'business', 'business', 'business', 'world', 'business', 'world', 'sports', 'sports', 'entertainment', 'sports', 'sports', 'science-technology', 'business', 'business', 'business', 'world', 'sports', 'sports', 'business', 'world', 'sports', 'science-technology', 'world', 'sports', 'sports', 'sports', 'business', 'sports', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'world', 'science-technology', 'world', 'sports', 'entertainment', 'business', 'sports', 'business', 'sports', 'business', 'science-technology', 'world', 'entertainment']\n"
     ]
    }
   ],
   "source": [
    "bow = BagOfWords()\n",
    "bow.fit(X_train)\n",
    "X_train_bow = bow.transform(X_train)\n",
    "X_test_bow = bow.transform(X_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow, y_train, len(bow.vocabulary))\n",
    "predictions = nb.predict_batch(X_test_bow)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model accuracy and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9642857142857143\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          business       1.00      0.97      0.99        73\n",
      "     entertainment       0.95      0.99      0.97        72\n",
      "science-technology       0.95      0.97      0.96        60\n",
      "            sports       1.00      0.98      0.99        66\n",
      "             world       0.92      0.91      0.91        65\n",
      "\n",
      "          accuracy                           0.96       336\n",
      "         macro avg       0.96      0.96      0.96       336\n",
      "      weighted avg       0.96      0.96      0.96       336\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[71  0  0  0  2]\n",
      " [ 0 71  0  0  1]\n",
      " [ 0  1 58  0  1]\n",
      " [ 0  0  0 65  1]\n",
      " [ 0  3  3  0 59]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
